Consideration of iron-chelation (IC) in transfusion-dependent patients is recommended in most clinical-practice guidelines on myelodysplastic syndromes (MDS). The financial impact of IC on health-care systems is predicted through economic modeling, but an analysis based on actual prevalence is lacking. Here, we have investigated the potential drug-costs and need for IC in a cohort of 189 United Kingdom-based MDS patients diagnosed from 2000 to 2010. Patients with low or intermediate-1 IPSS scores were identified as eligible for IC if ≥24 red cell units (RCU) had been transfused over 12 consecutive months or the transfusion-intensity averaged ≥2 RCU per month. Drug-costs were calculated from the time patients qualified for IC until death or last follow-up. In 159 patients with low/intermediate-1 MDS, survival was superior with a low IPSS score (P = 0.014), age <70 years (P = 0.043), transfusion-independence at diagnosis (P = 0.0056) and transfusion-intensity of <2 RCU per month (P = 0.009). Reflecting the time elapsed since diagnosis, longer survival was observed with a cumulative red cell load of ≥75 U (P = 0.046). By logistic-regression analysis, transfusion-intensity independently predicted survival (P = 0.0035) in low and intermediate-1 risk MDS patients. Forty-one patients fulfilled criteria for consideration of IC. Of these, 6 patients died within 1 month; 35 patients survived for a median of 16 months (range 1-61). Had patients commenced IC, the anticipated drug-costs alone would have been ~$526,880-$2,064,800 over 10 years. The lack of association between cumulative transfusion-load and survival calls for a prospective evaluation of the cost-utility of IC in patients surviving long-term, to enable evidence-based recommendations in MDS management.