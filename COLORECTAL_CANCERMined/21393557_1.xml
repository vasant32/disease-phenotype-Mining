<document xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:z="http://www.ebi.ac.uk/z" source="Whatizit"><text><SENT sid="0" pm="."><plain>BACKGROUND: Difficulty identifying patients in need of <z:e sem="disease" ids="C1527249,C0009402" disease_type="Neoplastic Process" abbrv="">colorectal cancer</z:e> (<z:e sem="disease" ids="AmbiguousAbbrv" disease_type="AmbiguousAbbrv" abbrv="ambiguous">CRC</z:e>) screening contributes to low screening rates </plain></SENT>
<SENT sid="1" pm="."><plain>OBJECTIVE: To use Electronic Health Record (EHR) data to identify patients with prior <z:e sem="disease" ids="AmbiguousAbbrv" disease_type="AmbiguousAbbrv" abbrv="ambiguous">CRC</z:e> testing </plain></SENT>
<SENT sid="2" pm="."><plain>DESIGN: A clinical natural language processing (NLP) system was modified to identify 4 CRC tests (colonoscopy, flexible sigmoidoscopy, fecal occult blood testing, and double contrast <z:chebi fb="8" ids="32594,32595">barium</z:chebi> enema) within electronic clinical documentation </plain></SENT>
<SENT sid="3" pm="."><plain>Text phrases in clinical notes referencing <z:e sem="disease" ids="AmbiguousAbbrv" disease_type="AmbiguousAbbrv" abbrv="ambiguous">CRC</z:e> tests were interpreted by the system to determine whether testing was planned or completed and to estimate the date of completed tests </plain></SENT>
<SENT sid="4" pm="."><plain>SETTING: Large academic medical center </plain></SENT>
<SENT sid="5" pm="."><plain>PATIENTS: 200 patients ≥ 50 years old who had completed ≥ 2 non-<z:hpo ids='HP_0011009'>acute</z:hpo> primary care visits within a 1-year period </plain></SENT>
<SENT sid="6" pm="."><plain>MEASURES: Recall and precision of the NLP system, billing records, and human chart review were compared to a reference standard of human review of <z:hpo ids='HP_0000001'>all</z:hpo> available information sources </plain></SENT>
<SENT sid="7" pm="."><plain>RESULTS: For identification of <z:hpo ids='HP_0000001'>all</z:hpo> <z:e sem="disease" ids="AmbiguousAbbrv" disease_type="AmbiguousAbbrv" abbrv="ambiguous">CRC</z:e> tests, recall and precision were as follows: NLP system (recall 93%, precision 94%), chart review (74%, 98%), and billing records review (44%, 83%) </plain></SENT>
<SENT sid="8" pm="."><plain>Recall and precision for identification of patients in need of screening were: NLP system (recall 95%, precision 88%), chart review (99%, 82%), and billing records (99%, 67%) </plain></SENT>
<SENT sid="9" pm="."><plain>LIMITATIONS: Small sample size and requirement for a robust EHR </plain></SENT>
<SENT sid="10" pm="."><plain>CONCLUSIONS: Applying NLP to EHR records detected more <z:e sem="disease" ids="AmbiguousAbbrv" disease_type="AmbiguousAbbrv" abbrv="ambiguous">CRC</z:e> tests than either manual chart review or billing records review alone </plain></SENT>
<SENT sid="11" pm="."><plain>NLP had better precision but marginally lower recall to identify patients who were due for <z:e sem="disease" ids="AmbiguousAbbrv" disease_type="AmbiguousAbbrv" abbrv="ambiguous">CRC</z:e> screening than billing record review </plain></SENT>
</text></document>