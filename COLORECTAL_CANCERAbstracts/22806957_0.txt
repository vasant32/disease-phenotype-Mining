Regression-based relative survival models are commonly used in population-based cancer studies to estimate the real impact on the excess mortality of covariates that influence overall mortality. Usually, the mortality observed in a study cohort is corrected by the expected mortality hazard in the general population, which is given by life tables provided by national statistics institutes. These life tables are stratified by age, sex, calendar year, and, sometimes, other demographic data (ethnicity, deprivation, and others). However, in most cases, the same demographic data are not available for the study cohort and the general population; this leads to differences between the expected mortality of the general population and that of the study cohort. More generally, the absence of some demographic variables in life tables may introduce a measurement bias into the estimation of the excess mortality. In the present article, we used a simulation approach with different plausible scenarios to evaluate the impact of an additional life-table variable on excess mortality estimates and study the extent and the direction of the biases in estimating the effect of each covariate on the excess mortality. We showed that the use of life table that lacks stratification by a variable present in the excess hazard model results in a measurement bias not only in the estimate of the effect of this variable but also, to a lesser extent, in the estimates of the effects of the other covariates included in the model. We also demonstrated this measurement bias by a population-based colorectal cancer analysis.